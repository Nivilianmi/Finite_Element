{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.thing = nn.Parameter(torch.rand(10), requires_grad=True)\n",
    "    \n",
    "#     def forward(self):\n",
    "#         return self.thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = Net()\n",
    "\n",
    "# learning_rate = 0.1\n",
    "\n",
    "# loss_fn = nn.MSELoss()\n",
    "\n",
    "# optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ideal_weights = torch.tensor([0.0027] * 10)\n",
    "# for epoch in range(100):\n",
    "#     optimizer.zero_grad()\n",
    "#     pred = net()\n",
    "#     print(pred)\n",
    "\n",
    "#     weight = torch.zeros(10)\n",
    "#     for i in range(10):\n",
    "#         if pred[i].item() > 0.5:\n",
    "#             weight[i] = 0.0027\n",
    "#         else:\n",
    "#             weight[i] = 0.00783\n",
    "    \n",
    "#     total_loss = ideal_weights - weight\n",
    "#     print(ideal_weights)\n",
    "#     print(weight)\n",
    "#     print(total_loss)\n",
    "\n",
    "#     loss = loss_fn(pred, total_loss.float())\n",
    "#     print(loss)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class customAutograd(torch.autograd.Function):\n",
    "#     @staticmethod\n",
    "#     def forward(ctx, input):\n",
    "#         \"\"\"\n",
    "#         In the forward pass we receive a Tensor containing the input and return\n",
    "#         a Tensor containing the output. ctx is a context object that can be used\n",
    "#         to stash information for backward computation. You can cache arbitrary\n",
    "#         objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "#         \"\"\"\n",
    "#         ctx.save_for_backward(input)\n",
    "#         return input / 2\n",
    "\n",
    "#     @staticmethod\n",
    "#     def backward(ctx, grad_output):\n",
    "#         \"\"\"\n",
    "#         In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "#         with respect to the output, and we need to compute the gradient of the loss\n",
    "#         with respect to the input.\n",
    "#         \"\"\"\n",
    "#         input, = ctx.saved_tensors\n",
    "#         return grad_output * input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class testNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(testNet, self).__init__()\n",
    "#         torch.manual_seed(12)\n",
    "#         self.elem_material = nn.Parameter(torch.rand(10), requires_grad=True)\n",
    "    \n",
    "#     def forward(self):\n",
    "#         print(self.elem_material)\n",
    "\n",
    "#         E_vector = 131 / (1 + torch.exp(-10 * (self.elem_material - torch.tensor([0.5] * 10)))) + 69\n",
    "#         Stif = E_vector * 10\n",
    "#         u = Stif / 2\n",
    "\n",
    "#         return u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = testNet()\n",
    "\n",
    "# num_epochs = 10\n",
    "# learning_rate = 1\n",
    "# loss_fn = nn.MSELoss()\n",
    "# optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# ideal_u = torch.tensor([5.0] * 10)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     optimizer.zero_grad()\n",
    "#     u = net()\n",
    "\n",
    "#     loss = loss_fn(u, ideal_u)\n",
    "#     loss.backward()\n",
    "\n",
    "#     for name, param in net.named_parameters():\n",
    "#         if param.requires_grad:\n",
    "#             print(name, param.grad)\n",
    "\n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class testFunc(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, mat):\n",
    "        ctx.save_for_backward(mat)\n",
    "\n",
    "        return torch.linalg.inv(mat)\n",
    "    \n",
    "    def backward(ctx, grad_output):\n",
    "        mat = ctx.saved_tensors[0]\n",
    "        print(mat)\n",
    "        matKron = torch.tensor(torch.kron(mat, mat), requires_grad=True)\n",
    "        matKron.backward(torch.ones_like(matKron))\n",
    "    \n",
    "        return torch.mm(grad_output, matKron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFunction = testFunc.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class testNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(testNet, self).__init__()\n",
    "        self.mat = nn.Parameter(torch.rand((20, 20), requires_grad=True, device=device))\n",
    "\n",
    "    def forward(self):    \n",
    "        return testFunction(self.mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.9085, 0.6041, 0.9862, 0.8138, 0.0299, 0.8591, 0.7446, 0.6788, 0.5976,\n",
      "         0.2402, 0.1613, 0.1781, 0.9200, 0.4087, 0.2358, 0.3741, 0.8329, 0.7011,\n",
      "         0.3213, 0.7330],\n",
      "        [0.6542, 0.9598, 0.9757, 0.4503, 0.6453, 0.8637, 0.3598, 0.1752, 0.0708,\n",
      "         0.4350, 0.3135, 0.0548, 0.9126, 0.0697, 0.3114, 0.6958, 0.5950, 0.6693,\n",
      "         0.2519, 0.7557],\n",
      "        [0.6775, 0.9440, 0.6960, 0.2229, 0.7594, 0.2043, 0.8151, 0.4815, 0.0986,\n",
      "         0.7851, 0.4147, 0.8952, 0.9937, 0.0763, 0.3401, 0.4406, 0.6369, 0.6385,\n",
      "         0.4158, 0.8581],\n",
      "        [0.6103, 0.8722, 0.3648, 0.6447, 0.3303, 0.4596, 0.3032, 0.7894, 0.8735,\n",
      "         0.7091, 0.4641, 0.2607, 0.5981, 0.6969, 0.6419, 0.9279, 0.0665, 0.3107,\n",
      "         0.9783, 0.0724],\n",
      "        [0.0484, 0.7685, 0.5785, 0.8047, 0.2464, 0.3336, 0.4751, 0.7594, 0.5347,\n",
      "         0.6532, 0.8528, 0.3441, 0.7532, 0.2871, 0.2056, 0.5964, 0.2061, 0.6672,\n",
      "         0.2814, 0.1722],\n",
      "        [0.7590, 0.7923, 0.1534, 0.1288, 0.6397, 0.5152, 0.5523, 0.9067, 0.8594,\n",
      "         0.7031, 0.8677, 0.5728, 0.2936, 0.6824, 0.2568, 0.7164, 0.2090, 0.5169,\n",
      "         0.4043, 0.7572],\n",
      "        [0.6845, 0.4155, 0.6990, 0.4458, 0.3001, 0.6151, 0.5435, 0.5729, 0.7606,\n",
      "         0.0482, 0.6079, 0.9547, 0.8125, 0.5501, 0.9938, 0.9206, 0.2880, 0.8373,\n",
      "         0.8960, 0.9523],\n",
      "        [0.0598, 0.4416, 0.2603, 0.0532, 0.1460, 0.4508, 0.6896, 0.6793, 0.5634,\n",
      "         0.7848, 0.3657, 0.7009, 0.0601, 0.3894, 0.0322, 0.1918, 0.4652, 0.8546,\n",
      "         0.4011, 0.1118],\n",
      "        [0.3451, 0.3513, 0.4447, 0.4203, 0.0706, 0.3433, 0.4280, 0.5788, 0.2215,\n",
      "         0.4653, 0.6227, 0.4608, 0.8053, 0.3900, 0.6722, 0.5135, 0.0022, 0.8417,\n",
      "         0.7612, 0.5502],\n",
      "        [0.3185, 0.1884, 0.4285, 0.3584, 0.8201, 0.1514, 0.0944, 0.4169, 0.2465,\n",
      "         0.9646, 0.2121, 0.8477, 0.7389, 0.7466, 0.1208, 0.6606, 0.9403, 0.3720,\n",
      "         0.3511, 0.5786],\n",
      "        [0.0060, 0.4072, 0.2973, 0.2368, 0.3472, 0.6116, 0.1819, 0.2000, 0.5112,\n",
      "         0.9354, 0.5417, 0.3143, 0.4831, 0.2955, 0.4361, 0.1672, 0.9752, 0.9128,\n",
      "         0.6472, 0.1079],\n",
      "        [0.0785, 0.2877, 0.5030, 0.0492, 0.7872, 0.0786, 0.7164, 0.0947, 0.4502,\n",
      "         0.2001, 0.7706, 0.0431, 0.1915, 0.3371, 0.7088, 0.9636, 0.0089, 0.2078,\n",
      "         0.9456, 0.6808],\n",
      "        [0.2579, 0.1311, 0.3459, 0.5314, 0.2532, 0.0105, 0.2940, 0.9903, 0.6461,\n",
      "         0.8280, 0.7126, 0.5311, 0.1864, 0.3257, 0.6474, 0.4861, 0.2486, 0.4610,\n",
      "         0.8964, 0.7025],\n",
      "        [0.7757, 0.4320, 0.9776, 0.7046, 0.6432, 0.6009, 0.6429, 0.1948, 0.3178,\n",
      "         0.5546, 0.9709, 0.8542, 0.9916, 0.3538, 0.1848, 0.2866, 0.1981, 0.1454,\n",
      "         0.7777, 0.8058],\n",
      "        [0.8068, 0.2390, 0.7454, 0.8366, 0.3913, 0.4951, 0.9871, 0.0301, 0.4052,\n",
      "         0.9220, 0.8281, 0.1843, 0.8331, 0.2130, 0.4641, 0.8632, 0.7296, 0.8566,\n",
      "         0.6712, 0.1394],\n",
      "        [0.0278, 0.8572, 0.6461, 0.6879, 0.5108, 0.5119, 0.1194, 0.2875, 0.4156,\n",
      "         0.7771, 0.4350, 0.6235, 0.5039, 0.1981, 0.9217, 0.5767, 0.1552, 0.6470,\n",
      "         0.8337, 0.5618],\n",
      "        [0.8210, 0.1064, 0.2296, 0.1340, 0.1107, 0.6011, 0.2134, 0.3160, 0.8824,\n",
      "         0.5389, 0.5588, 0.9112, 0.5333, 0.1395, 0.9566, 0.1098, 0.1961, 0.4967,\n",
      "         0.5033, 0.6976],\n",
      "        [0.5170, 0.1303, 0.3533, 0.0756, 0.5560, 0.0251, 0.3519, 0.3767, 0.8291,\n",
      "         0.6819, 0.6381, 0.3398, 0.1180, 0.1237, 0.3500, 0.4195, 0.6533, 0.5409,\n",
      "         0.8707, 0.0860],\n",
      "        [0.0581, 0.2824, 0.5678, 0.6559, 0.9875, 0.4992, 0.5121, 0.0820, 0.5445,\n",
      "         0.6674, 0.8585, 0.9273, 0.8709, 0.2352, 0.3704, 0.1348, 0.9629, 0.8754,\n",
      "         0.6849, 0.3852],\n",
      "        [0.9257, 0.1740, 0.2572, 0.5780, 0.2671, 0.4903, 0.0411, 0.6131, 0.3069,\n",
      "         0.7689, 0.0985, 0.4088, 0.5529, 0.4391, 0.8112, 0.9985, 0.2573, 0.8021,\n",
      "         0.9940, 0.3541]], device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hyech\\AppData\\Local\\Temp\\ipykernel_19920\\3762948969.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  matKron = torch.tensor(torch.kron(mat, mat), requires_grad=True)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (20x20 and 400x400)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [12], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m output \u001b[38;5;241m=\u001b[39m net()\n\u001b[0;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output, target)\n\u001b[1;32m---> 11\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\autograd\\function.py:253\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImplementing both \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbackward\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvjp\u001b[39m\u001b[39m'\u001b[39m\u001b[39m for a custom \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mFunction is not allowed. You should only implement one \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    251\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mof them.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    252\u001b[0m user_fn \u001b[39m=\u001b[39m vjp_fn \u001b[39mif\u001b[39;00m vjp_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m Function\u001b[39m.\u001b[39mvjp \u001b[39melse\u001b[39;00m backward_fn\n\u001b[1;32m--> 253\u001b[0m \u001b[39mreturn\u001b[39;00m user_fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs)\n",
      "Cell \u001b[1;32mIn [9], line 14\u001b[0m, in \u001b[0;36mtestFunc.backward\u001b[1;34m(ctx, grad_output)\u001b[0m\n\u001b[0;32m     11\u001b[0m matKron \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(torch\u001b[38;5;241m.\u001b[39mkron(mat, mat), requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m matKron\u001b[38;5;241m.\u001b[39mbackward(torch\u001b[38;5;241m.\u001b[39mones_like(matKron))\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatKron\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (20x20 and 400x400)"
     ]
    }
   ],
   "source": [
    "net = testNet()\n",
    "net.to(device)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "target = torch.zeros((20, 20), device=device)\n",
    "\n",
    "for epoch in range(10):\n",
    "    output = net()\n",
    "    loss = loss_fn(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10]) (10, 1) \n",
      " tensor([[9.3497e-01, 4.0005e-01, 2.0105e-02, 2.6490e-01, 9.8470e-01, 4.3822e-01,\n",
      "         2.0894e-02, 2.5077e-01, 9.6684e-01, 1.2306e-01],\n",
      "        [7.8026e-01, 9.2803e-01, 4.2527e-01, 1.5272e-01, 7.1347e-01, 1.0540e-01,\n",
      "         7.2966e-02, 2.8718e-01, 4.1051e-01, 9.5797e-02],\n",
      "        [9.8772e-01, 1.8273e-01, 1.3484e-01, 8.2902e-01, 8.0842e-04, 4.5023e-01,\n",
      "         3.2139e-01, 1.3349e-01, 8.9626e-01, 2.3043e-01],\n",
      "        [8.5540e-02, 2.2020e-01, 1.1609e-01, 1.4752e-01, 4.2841e-01, 2.3119e-02,\n",
      "         7.2707e-01, 1.2080e-01, 8.3656e-01, 2.2331e-01],\n",
      "        [9.5180e-01, 6.1940e-01, 9.5660e-01, 2.1658e-01, 9.4941e-01, 3.4822e-01,\n",
      "         7.8551e-01, 6.2152e-01, 6.1128e-01, 5.7465e-01],\n",
      "        [6.1736e-01, 8.8028e-01, 2.0126e-01, 4.2004e-01, 5.1344e-01, 6.3661e-01,\n",
      "         5.6108e-01, 7.3427e-01, 8.9320e-01, 9.4737e-01],\n",
      "        [5.8891e-01, 4.4999e-01, 4.4679e-01, 5.0741e-01, 6.7626e-01, 3.7722e-01,\n",
      "         7.1011e-01, 8.5262e-02, 2.1999e-01, 4.5742e-01],\n",
      "        [7.8566e-01, 3.8090e-01, 9.8396e-01, 9.6076e-01, 5.9398e-01, 7.1266e-01,\n",
      "         2.0467e-01, 2.0314e-01, 9.1616e-02, 4.3249e-01],\n",
      "        [4.0991e-01, 4.7581e-01, 7.1149e-01, 8.4157e-01, 3.4496e-01, 3.6747e-01,\n",
      "         9.3034e-02, 9.2195e-01, 2.0019e-01, 9.7036e-01],\n",
      "        [6.7322e-02, 1.7315e-01, 5.3862e-01, 7.1273e-01, 7.3170e-01, 5.3989e-01,\n",
      "         7.2625e-01, 4.4976e-01, 1.1562e-01, 8.0064e-01]])\n",
      "torch.Size([10, 10]) (1, 10) \n",
      " tensor([[ 0.3368, -0.5558,  0.4467, -0.9914,  0.6196, -0.3115,  1.4170, -0.7405,\n",
      "          0.4081, -0.8176],\n",
      "        [-0.7838,  1.5023,  0.0079,  0.1079, -0.6601,  0.6315, -0.3292,  0.2008,\n",
      "         -0.4954,  0.3150],\n",
      "        [-0.1550, -0.9176, -0.9849,  1.2746,  0.4148,  0.0810, -0.1136,  1.6380,\n",
      "          0.4183, -1.6589],\n",
      "        [-0.3162,  1.3652,  1.0679,  0.0324, -0.8477, -0.8828, -0.4559, -0.4890,\n",
      "          0.1818,  1.5260],\n",
      "        [ 0.9206,  0.0106, -0.5947,  0.1284, -0.2481, -0.5789,  0.5815, -0.2359,\n",
      "          0.3048,  0.2815],\n",
      "        [-0.4091,  0.5232,  0.5408, -1.1961,  0.3723,  1.3057, -2.5958,  0.8728,\n",
      "         -2.2332,  2.0842],\n",
      "        [-1.0548,  1.2952,  1.4032, -0.8260,  0.6753, -0.2009, -0.9858, -1.1422,\n",
      "         -1.3163,  2.3623],\n",
      "        [-1.0013,  2.9218,  2.4149, -2.1246,  1.1890, -0.5505, -4.2049, -1.9997,\n",
      "         -1.4997,  4.8000],\n",
      "        [ 0.4733, -0.9746, -0.6621,  1.6597, -0.2521,  0.3343, -0.0361,  0.9943,\n",
      "          0.5122, -1.5803],\n",
      "        [ 1.4124, -3.9142, -2.6824,  1.5660, -0.7022,  0.7128,  4.9149,  0.9951,\n",
      "          2.8194, -5.2665]])\n",
      "torch.Size([10, 10]) (10, 1) \n",
      " tensor([[ 0.3368, -0.5558,  0.4467, -0.9914,  0.6196, -0.3115,  1.4170, -0.7405,\n",
      "          0.4081, -0.8176],\n",
      "        [-0.7838,  1.5023,  0.0079,  0.1079, -0.6601,  0.6315, -0.3292,  0.2008,\n",
      "         -0.4954,  0.3150],\n",
      "        [-0.1550, -0.9176, -0.9849,  1.2746,  0.4148,  0.0810, -0.1136,  1.6380,\n",
      "          0.4183, -1.6589],\n",
      "        [-0.3162,  1.3652,  1.0679,  0.0324, -0.8477, -0.8828, -0.4559, -0.4890,\n",
      "          0.1818,  1.5260],\n",
      "        [ 0.9206,  0.0106, -0.5947,  0.1284, -0.2481, -0.5789,  0.5815, -0.2359,\n",
      "          0.3048,  0.2815],\n",
      "        [-0.4091,  0.5232,  0.5408, -1.1961,  0.3723,  1.3057, -2.5958,  0.8728,\n",
      "         -2.2332,  2.0842],\n",
      "        [-1.0548,  1.2952,  1.4032, -0.8260,  0.6753, -0.2009, -0.9858, -1.1422,\n",
      "         -1.3163,  2.3623],\n",
      "        [-1.0013,  2.9218,  2.4149, -2.1246,  1.1890, -0.5505, -4.2049, -1.9997,\n",
      "         -1.4997,  4.8000],\n",
      "        [ 0.4733, -0.9746, -0.6621,  1.6597, -0.2521,  0.3343, -0.0361,  0.9943,\n",
      "          0.5122, -1.5803],\n",
      "        [ 1.4124, -3.9142, -2.6824,  1.5660, -0.7022,  0.7128,  4.9149,  0.9951,\n",
      "          2.8194, -5.2665]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((10, 10))\n",
    "y = torch.linalg.inv(x)\n",
    "z = torch.zeros((10, 10))\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        z[i][j] = y[i][j]\n",
    "print(x.shape, x.stride(), '\\n', x)\n",
    "print(y.shape, y.stride(), '\\n', y)\n",
    "print(z.shape, z.stride(), '\\n', z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "87e71b922caa86329377ca3f8aa0dceb4b1830998398065218a797cfca03030a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
