{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.thing = nn.Parameter(torch.rand(10), requires_grad=True)\n",
    "    \n",
    "#     def forward(self):\n",
    "#         return self.thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = Net()\n",
    "\n",
    "# learning_rate = 0.1\n",
    "\n",
    "# loss_fn = nn.MSELoss()\n",
    "\n",
    "# optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ideal_weights = torch.tensor([0.0027] * 10)\n",
    "# for epoch in range(100):\n",
    "#     optimizer.zero_grad()\n",
    "#     pred = net()\n",
    "#     print(pred)\n",
    "\n",
    "#     weight = torch.zeros(10)\n",
    "#     for i in range(10):\n",
    "#         if pred[i].item() > 0.5:\n",
    "#             weight[i] = 0.0027\n",
    "#         else:\n",
    "#             weight[i] = 0.00783\n",
    "    \n",
    "#     total_loss = ideal_weights - weight\n",
    "#     print(ideal_weights)\n",
    "#     print(weight)\n",
    "#     print(total_loss)\n",
    "\n",
    "#     loss = loss_fn(pred, total_loss.float())\n",
    "#     print(loss)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class customAutograd(torch.autograd.Function):\n",
    "#     @staticmethod\n",
    "#     def forward(ctx, input):\n",
    "#         \"\"\"\n",
    "#         In the forward pass we receive a Tensor containing the input and return\n",
    "#         a Tensor containing the output. ctx is a context object that can be used\n",
    "#         to stash information for backward computation. You can cache arbitrary\n",
    "#         objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "#         \"\"\"\n",
    "#         ctx.save_for_backward(input)\n",
    "#         return input / 2\n",
    "\n",
    "#     @staticmethod\n",
    "#     def backward(ctx, grad_output):\n",
    "#         \"\"\"\n",
    "#         In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "#         with respect to the output, and we need to compute the gradient of the loss\n",
    "#         with respect to the input.\n",
    "#         \"\"\"\n",
    "#         input, = ctx.saved_tensors\n",
    "#         return grad_output * input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class testNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(testNet, self).__init__()\n",
    "#         torch.manual_seed(12)\n",
    "#         self.elem_material = nn.Parameter(torch.rand(10), requires_grad=True)\n",
    "    \n",
    "#     def forward(self):\n",
    "#         print(self.elem_material)\n",
    "\n",
    "#         E_vector = 131 / (1 + torch.exp(-10 * (self.elem_material - torch.tensor([0.5] * 10)))) + 69\n",
    "#         Stif = E_vector * 10\n",
    "#         u = Stif / 2\n",
    "\n",
    "#         return u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = testNet()\n",
    "\n",
    "# num_epochs = 10\n",
    "# learning_rate = 1\n",
    "# loss_fn = nn.MSELoss()\n",
    "# optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)\n",
    "\n",
    "# ideal_u = torch.tensor([5.0] * 10)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     optimizer.zero_grad()\n",
    "#     u = net()\n",
    "\n",
    "#     loss = loss_fn(u, ideal_u)\n",
    "#     loss.backward()\n",
    "\n",
    "#     for name, param in net.named_parameters():\n",
    "#         if param.requires_grad:\n",
    "#             print(name, param.grad)\n",
    "\n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class testFunc(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, mat):\n",
    "        ctx.save_for_backward(mat)\n",
    "\n",
    "        return torch.linalg.inv(mat)\n",
    "    \n",
    "    def backward(ctx, grad_output):\n",
    "        mat = ctx.saved_tensors[0]\n",
    "        print(mat)\n",
    "        matKron = torch.tensor(torch.kron(mat, mat), requires_grad=True)\n",
    "        matKron.backward(torch.ones_like(matKron))\n",
    "    \n",
    "        return torch.mm(grad_output, matKron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFunction = testFunc.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class testNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(testNet, self).__init__()\n",
    "        self.mat = nn.Parameter(torch.rand((20, 20), requires_grad=True, device=device))\n",
    "\n",
    "    def forward(self):    \n",
    "        return testFunction(self.mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = testNet()\n",
    "# net.to(device)\n",
    "# optimizer = torch.optim.SGD(net.parameters(), lr=0.1)\n",
    "# loss_fn = nn.MSELoss()\n",
    "\n",
    "# target = torch.zeros((20, 20), device=device)\n",
    "\n",
    "# for epoch in range(10):\n",
    "#     output = net()\n",
    "#     loss = loss_fn(output, target)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10]) (10, 1) \n",
      " tensor([[0.1918, 0.1803, 0.2155, 0.5611, 0.5720, 0.9786, 0.6170, 0.3775, 0.4423,\n",
      "         0.5906],\n",
      "        [0.6288, 0.0203, 0.6156, 0.9616, 0.5464, 0.6602, 0.5558, 0.8209, 0.6746,\n",
      "         0.7378],\n",
      "        [0.6018, 0.2573, 0.3906, 0.1765, 0.8904, 0.5973, 0.1033, 0.6991, 0.7451,\n",
      "         0.1207],\n",
      "        [0.5554, 0.1747, 0.7398, 0.0433, 0.8002, 0.5523, 0.8501, 0.4919, 0.0850,\n",
      "         0.3248],\n",
      "        [0.6316, 0.1739, 0.6015, 0.1284, 0.7329, 0.7275, 0.9069, 0.5207, 0.1282,\n",
      "         0.3518],\n",
      "        [0.9457, 0.8961, 0.2276, 0.1977, 0.5779, 0.2690, 0.7003, 0.3062, 0.7441,\n",
      "         0.1640],\n",
      "        [0.9599, 0.7538, 0.9345, 0.6052, 0.3923, 0.8109, 0.9056, 0.3868, 0.7032,\n",
      "         0.8897],\n",
      "        [0.5704, 0.9538, 0.6355, 0.8186, 0.2495, 0.7299, 0.5380, 0.8492, 0.8569,\n",
      "         0.8880],\n",
      "        [0.6319, 0.7336, 0.0688, 0.3943, 0.4307, 0.6660, 0.1119, 0.4437, 0.7585,\n",
      "         0.6704],\n",
      "        [0.3222, 0.4476, 0.0326, 0.9590, 0.5586, 0.8565, 0.2932, 0.9464, 0.0746,\n",
      "         0.2035]])\n",
      "torch.Size([10, 10]) (1, 10) \n",
      " tensor([[-1.4267,  0.5218, -0.0701, -1.7221,  2.0118, -0.0532,  0.7678, -1.0914,\n",
      "          0.8389,  0.2459],\n",
      "        [ 0.1060, -0.9395, -0.1730,  1.5123, -1.7427,  0.2601,  0.2561,  0.3608,\n",
      "          0.1189,  0.5047],\n",
      "        [-0.1381, -0.4971,  0.9320,  0.9988, -1.5434, -0.6213,  1.4026,  0.0929,\n",
      "         -1.0560,  0.1673],\n",
      "        [ 0.4182,  0.5479, -0.2274,  1.1515, -2.6066,  0.4121,  0.9212, -0.8975,\n",
      "         -0.5298,  0.9052],\n",
      "        [ 0.7409,  0.2534, -0.2582,  3.9852, -4.0296,  0.4112, -0.2362, -1.0702,\n",
      "          0.8199,  0.3602],\n",
      "        [ 0.3940, -1.0727,  0.9998, -2.6473,  2.3754, -0.8184,  1.0458, -0.0826,\n",
      "         -0.4587,  0.2305],\n",
      "        [ 0.6218,  0.2364, -0.6762, -0.7997,  1.4520,  0.8920, -0.9429,  0.7141,\n",
      "         -0.8234, -0.4946],\n",
      "        [-0.9913,  0.3019, -0.0086, -1.3490,  2.7520, -0.2258, -1.6587,  1.5435,\n",
      "          0.0465, -0.2720],\n",
      "        [ 0.7608,  0.1523,  0.8569, -1.5144,  0.6761,  0.5045, -0.3882,  0.7251,\n",
      "         -0.9150, -0.8795],\n",
      "        [-0.1790,  0.7561, -1.3846,  1.9977, -0.8975, -0.4346, -0.8824, -0.0392,\n",
      "          2.0615, -0.5364]])\n",
      "torch.Size([10, 10]) (10, 1) \n",
      " tensor([[-1.4267,  0.5218, -0.0701, -1.7221,  2.0118, -0.0532,  0.7678, -1.0914,\n",
      "          0.8389,  0.2459],\n",
      "        [ 0.1060, -0.9395, -0.1730,  1.5123, -1.7427,  0.2601,  0.2561,  0.3608,\n",
      "          0.1189,  0.5047],\n",
      "        [-0.1381, -0.4971,  0.9320,  0.9988, -1.5434, -0.6213,  1.4026,  0.0929,\n",
      "         -1.0560,  0.1673],\n",
      "        [ 0.4182,  0.5479, -0.2274,  1.1515, -2.6066,  0.4121,  0.9212, -0.8975,\n",
      "         -0.5298,  0.9052],\n",
      "        [ 0.7409,  0.2534, -0.2582,  3.9852, -4.0296,  0.4112, -0.2362, -1.0702,\n",
      "          0.8199,  0.3602],\n",
      "        [ 0.3940, -1.0727,  0.9998, -2.6473,  2.3754, -0.8184,  1.0458, -0.0826,\n",
      "         -0.4587,  0.2305],\n",
      "        [ 0.6218,  0.2364, -0.6762, -0.7997,  1.4520,  0.8920, -0.9429,  0.7141,\n",
      "         -0.8234, -0.4946],\n",
      "        [-0.9913,  0.3019, -0.0086, -1.3490,  2.7520, -0.2258, -1.6587,  1.5435,\n",
      "          0.0465, -0.2720],\n",
      "        [ 0.7608,  0.1523,  0.8569, -1.5144,  0.6761,  0.5045, -0.3882,  0.7251,\n",
      "         -0.9150, -0.8795],\n",
      "        [-0.1790,  0.7561, -1.3846,  1.9977, -0.8975, -0.4346, -0.8824, -0.0392,\n",
      "          2.0615, -0.5364]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((10, 10))\n",
    "y = torch.linalg.inv(x)\n",
    "z = torch.zeros((10, 10))\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        z[i][j] = y[i][j]\n",
    "print(x.shape, x.stride(), '\\n', x)\n",
    "print(y.shape, y.stride(), '\\n', y)\n",
    "print(z.shape, z.stride(), '\\n', z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3,  4,  5,  6,  8, 10,  9, 12, 15])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat1 = torch.tensor([1, 2, 3])\n",
    "mat2 = torch.tensor([[3, 4, 5, 0, 0, 0, 0, 0, 0],\n",
    "                     [0, 0, 0, 3, 4, 5, 0, 0, 0],\n",
    "                     [0, 0, 0, 0, 0, 0, 3, 4, 5]])\n",
    "\n",
    "mat1 @ mat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.LongTensor{[9]}, size=[]): the number of sizes provided (0) must be greater or equal to the number of dimensions in the tensor (1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [16], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[0;32m      4\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[1;32m----> 5\u001b[0m         mat3[i][j \u001b[39m+\u001b[39;49m k] \u001b[39m=\u001b[39m mat2[j]\n\u001b[0;32m      6\u001b[0m     k \u001b[39m=\u001b[39m k \u001b[39m+\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m      7\u001b[0m mat3\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expand(torch.LongTensor{[9]}, size=[]): the number of sizes provided (0) must be greater or equal to the number of dimensions in the tensor (1)"
     ]
    }
   ],
   "source": [
    "mat3 = torch.zeros((10, 100))\n",
    "k = 0\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        mat3[i][j + k] = mat2[j]\n",
    "    k = k + 10\n",
    "mat3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manualKron(mat1, mat2):\n",
    "    initSize = 2\n",
    "    kronSize = initSize ** 2\n",
    "    kron = torch.zeros((kronSize, kronSize))\n",
    "    x = 0\n",
    "    for i in range(initSize):\n",
    "        for k in range(initSize):\n",
    "            y = 0\n",
    "            for j in range(initSize):\n",
    "                for l in range(initSize):\n",
    "                    kron[x][y] = mat1[i][j].item() * mat2[k][l].item()\n",
    "                    y += 1\n",
    "            x += 1\n",
    "    return kron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -0.2500,  0.5000, -0.2500],\n",
       "        [-0.2500, -4.0625,  2.8750,  0.4375],\n",
       "        [ 0.5000,  2.8750, -2.2500, -0.1250],\n",
       "        [-0.2500,  0.4375, -0.1250, -0.0625]], grad_fn=<LinalgInvExBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat1 = torch.tensor([[2.0, 2.0, 3.0, 0.0],\n",
    "                     [2.0, 3.0, 4.0, 5.0],\n",
    "                     [3.0, 4.0, 5.0, 6.0],\n",
    "                     [0.0, 5.0, 6.0, 7.0]], requires_grad=True)\n",
    "mat2 = torch.linalg.inv(mat1)\n",
    "mat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2]])\n",
      "tensor([[3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1],\n",
      "        [2]])\n",
      "tensor([[3, 5],\n",
      "        [4, 6]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  8],\n",
       "        [ 5, 12]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat1 = torch.tensor([[1, 2]])\n",
    "mat2 = torch.tensor([[3, 4], [5, 6]])\n",
    "print(mat1)\n",
    "print(mat2)\n",
    "print(torch.transpose(mat1, 0, 1))\n",
    "print(torch.transpose(mat2, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "fullString = input(\"Enter a string: \")\n",
    "num = input(\"Enter a number: \")\n",
    "\n",
    "uniqueString = set(fullString)\n",
    "uniqueString_list = []\n",
    "for k in uniqueString:\n",
    "    uniqueString_list.append(k)\n",
    "\n",
    "repChar = []\n",
    "\n",
    "for letter1 in uniqueString_list:\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for letter2 in fullString:\n",
    "        if letter2 == letter1:\n",
    "            count = count + 1\n",
    "\n",
    "    if count == num:\n",
    "        repChar.append(letter1)\n",
    "\n",
    "print(repChar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0000],\n",
       "        [ 1.0000]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "mat1 = torch.tensor([[1, 2], [3, 5]]).float()\n",
    "mat2 = torch.tensor([[1], [2]]).float()\n",
    "torch.linalg.solve(mat1, mat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 56\u001b[0m\n\u001b[0;32m     54\u001b[0m key_text \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPlease input the key: \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     55\u001b[0m genkey \u001b[39m=\u001b[39m generateKey(plain_text, key_text)\n\u001b[1;32m---> 56\u001b[0m encrypt \u001b[39m=\u001b[39m encrypt(plain_text, key_text)\n\u001b[0;32m     57\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mGenerated Key: \u001b[39m\u001b[39m\"\u001b[39m, genkey)\n\u001b[0;32m     58\u001b[0m \u001b[39mprint\u001b[39m((\u001b[39m\"\u001b[39m\u001b[39mCiphertext: \u001b[39m\u001b[39m\"\u001b[39m), encrypt)\n",
      "Cell \u001b[1;32mIn [4], line 37\u001b[0m, in \u001b[0;36mencrypt\u001b[1;34m(plain_text, generated_key)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39m# Looping through each character in the length of PT\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(plain_text)):\n\u001b[0;32m     35\u001b[0m     \u001b[39m# Turning each character of PT and KT into its ASCII values then\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     \u001b[39m# adding them and finding modulus to find remainder\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     v \u001b[39m=\u001b[39m (\u001b[39mord\u001b[39m(plain_text[i]) \u001b[39m+\u001b[39m \u001b[39mord\u001b[39m(generated_key[i])) \u001b[39m%\u001b[39m \u001b[39m128\u001b[39m\n\u001b[0;32m     38\u001b[0m     \u001b[39m# Adding the new value of v back into list one by one\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     cipher_text\u001b[39m.\u001b[39mappend(\u001b[39mchr\u001b[39m(v))\n",
      "\u001b[1;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "# CS 177\n",
    "# Lab: 07\n",
    "# Filename: lab07.py\n",
    "# Name: [Edith Azpeitia]\n",
    "# Purdue Email: eazpeiti@purdue.edu\n",
    "# Task description: Using Vigenere cipher \n",
    "# (input): A string and an integer key\n",
    "# (output): After repeating given keyword it encrypts it\n",
    "\n",
    "# Function 1\n",
    "def generateKey(plain_text, key_text):\n",
    "    # Making list of keytext/user input\n",
    "    key_list = list(key_text)\n",
    "    \n",
    "    # Making if else statement\n",
    "    # If keytext and plaintext are same normal keytext would be returned\n",
    "    if len(plain_text) == len(key_text):\n",
    "        return(key_text)\n",
    "\n",
    "    # Else statement for when the keytext doesn't have enough characters for plaintext \n",
    "    else:\n",
    "        for i in range(len(plain_text) - len(key_text)):\n",
    "            key_list.append(key_text[i % len(key_text)])\n",
    "            \n",
    "    # Joining to make string\n",
    "    return(\"\" . join(key_list))\n",
    "\n",
    "\n",
    "# Function 2\n",
    "def encrypt(plain_text, generated_key):\n",
    "    # Making an empty list\n",
    "    cipher_text = []\n",
    "    # Looping through each character in the length of PT\n",
    "    for i in range(len(plain_text)):\n",
    "        # Turning each character of PT and KT into its ASCII values then\n",
    "        # adding them and finding modulus to find remainder\n",
    "        v = (ord(plain_text[i]) + ord(generated_key[i])) % 128\n",
    "        # Adding the new value of v back into list one by one\n",
    "        cipher_text.append(chr(v))\n",
    "        # Joining character in list to make a string\n",
    "    return(\"\" . join(cipher_text))\n",
    "    \n",
    "# Function 3\n",
    "def decrypt(cipher_text, generated_key):\n",
    "    x = []\n",
    "    for i in range(len(cipher_text)):\n",
    "        v = (ord(cipher_text[i]) - ord(generated_key[i]) + 128) % 128\n",
    "        x.append(chr(v))\n",
    "    return(\"\" . join(x))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    plain_text = input(\"Please input the plaintext: \")\n",
    "    key_text = input(\"Please input the key: \")\n",
    "    genkey = generateKey(plain_text, key_text)\n",
    "    encrypt = encrypt(plain_text, key_text)\n",
    "    print(\"Generated Key: \", genkey)\n",
    "    print((\"Ciphertext: \"), encrypt)\n",
    "    print(\"Original/Decrypted Text:\", decrypt(encrypt, genkey))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.,  0., -3.,  0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "mat1 = [[-1, 0, -2, 0],\n",
    "        [0, -1, 0, -2],\n",
    "        [-3, 0, -4, 0],\n",
    "        [0, -3, 0, -4]]\n",
    "tens1 = torch.tensor(mat1).float()\n",
    "tens1trans = tens1.transpose(0, 1)\n",
    "tens1[0]\n",
    "tens1trans[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 4., 6., 8.])\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[1, 3],\n",
      "        [2, 4]])\n",
      "tensor([-1, -2, -3, -6])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "dot : expected both vectors to have same dtype, but found Float and Long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [16], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m         a \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     27\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 28\u001b[0m     final_grad[i] \u001b[39m=\u001b[39m dStif \u001b[39m@\u001b[39;49m kronCol\n\u001b[0;32m     29\u001b[0m \u001b[39mprint\u001b[39m(final_grad)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: dot : expected both vectors to have same dtype, but found Float and Long"
     ]
    }
   ],
   "source": [
    "grad_output = torch.tensor([1, 2])\n",
    "resid = torch.tensor([3, 4])\n",
    "mat2 = [[1, 2], [3, 4]]\n",
    "StifInv = torch.tensor(mat2).float()\n",
    "StifInvTrans = torch.transpose(StifInv, 0, 1)\n",
    "\n",
    "dStif = torch.zeros(4)\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        dStif[i * 2 + j] = grad_output[i] * resid[j]\n",
    "\n",
    "print(dStif)\n",
    "print(StifInv)\n",
    "print(StifInvTrans)\n",
    "\n",
    "final_grad = torch.zeros(4)\n",
    "a = 0\n",
    "b = 0\n",
    "for i in range(4):\n",
    "    kronCol = torch.kron(-StifInvTrans[a], StifInv[b])\n",
    "    print(kronCol)\n",
    "    b += 1\n",
    "    if b == 2:\n",
    "        a += 1\n",
    "        b = 0\n",
    "    final_grad[i] = dStif @ kronCol\n",
    "print(final_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "87e71b922caa86329377ca3f8aa0dceb4b1830998398065218a797cfca03030a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
