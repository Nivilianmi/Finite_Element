{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Net, self).__init__()\n",
    "#         self.thing = nn.Parameter(torch.rand(10), requires_grad=True)\n",
    "    \n",
    "#     def forward(self):\n",
    "#         return self.thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = Net()\n",
    "\n",
    "# learning_rate = 0.1\n",
    "\n",
    "# loss_fn = nn.MSELoss()\n",
    "\n",
    "# optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ideal_weights = torch.tensor([0.0027] * 10)\n",
    "# for epoch in range(100):\n",
    "#     optimizer.zero_grad()\n",
    "#     pred = net()\n",
    "#     print(pred)\n",
    "\n",
    "#     weight = torch.zeros(10)\n",
    "#     for i in range(10):\n",
    "#         if pred[i].item() > 0.5:\n",
    "#             weight[i] = 0.0027\n",
    "#         else:\n",
    "#             weight[i] = 0.00783\n",
    "    \n",
    "#     total_loss = ideal_weights - weight\n",
    "#     print(ideal_weights)\n",
    "#     print(weight)\n",
    "#     print(total_loss)\n",
    "\n",
    "#     loss = loss_fn(pred, total_loss.float())\n",
    "#     print(loss)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class customAutograd(torch.autograd.Function):\n",
    "#     @staticmethod\n",
    "#     def forward(ctx, input):\n",
    "#         \"\"\"\n",
    "#         In the forward pass we receive a Tensor containing the input and return\n",
    "#         a Tensor containing the output. ctx is a context object that can be used\n",
    "#         to stash information for backward computation. You can cache arbitrary\n",
    "#         objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "#         \"\"\"\n",
    "#         ctx.save_for_backward(input)\n",
    "#         return input / 2\n",
    "\n",
    "#     @staticmethod\n",
    "#     def backward(ctx, grad_output):\n",
    "#         \"\"\"\n",
    "#         In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "#         with respect to the output, and we need to compute the gradient of the loss\n",
    "#         with respect to the input.\n",
    "#         \"\"\"\n",
    "#         input, = ctx.saved_tensors\n",
    "#         return grad_output * input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "class testNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(testNet, self).__init__()\n",
    "        torch.manual_seed(12)\n",
    "        self.elem_material = nn.Parameter(torch.rand(10), requires_grad=True)\n",
    "    \n",
    "    def forward(self):\n",
    "        print(self.elem_material)\n",
    "\n",
    "        E_vector = 131 / (1 + torch.exp(-10 * (self.elem_material - torch.tensor([0.5] * 10)))) + 69\n",
    "        Stif = E_vector * 10\n",
    "        u = Stif / 2\n",
    "\n",
    "        return u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([0.4657, 0.2328, 0.4527, 0.5871, 0.4086, 0.1272, 0.6373, 0.2421, 0.7312,\n",
      "        0.7224], requires_grad=True)\n",
      "elem_material tensor([194599.2812,  30276.1992, 183279.0469, 218446.6406, 141193.3125,\n",
      "         10670.0410, 182258.2031,  33139.8281, 100560.4609, 107408.7109])\n",
      "Parameter containing:\n",
      "tensor([-194598.8125,  -30275.9668, -183278.5938, -218446.0469, -141192.9062,\n",
      "         -10669.9141, -182257.5625,  -33139.5859, -100559.7266, -107407.9922],\n",
      "       requires_grad=True)\n",
      "elem_material tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], requires_grad=True)\n",
      "elem_material tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], requires_grad=True)\n",
      "elem_material tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], requires_grad=True)\n",
      "elem_material tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], requires_grad=True)\n",
      "elem_material tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], requires_grad=True)\n",
      "elem_material tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], requires_grad=True)\n",
      "elem_material tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], requires_grad=True)\n",
      "elem_material tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n",
      "Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], requires_grad=True)\n",
      "elem_material tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])\n"
     ]
    }
   ],
   "source": [
    "net = testNet()\n",
    "\n",
    "num_epochs = 10\n",
    "learning_rate = 1\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)\n",
    "\n",
    "ideal_u = torch.tensor([5.0] * 10)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    u = net()\n",
    "\n",
    "    loss = loss_fn(u, ideal_u)\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in net.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(name, param.grad)\n",
    "\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
